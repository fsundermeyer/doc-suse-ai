// This file is part of the project https://github.com/openSUSE/doc-kit
// DO NOT EDIT THIS FILE DOWNSTREAM. IT MAY BE OVERWRITTEN BY AN UPDATE

:doctype: article
:experimental:
:docinfo:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

include::../common/generic-attributes.adoc[]

= Introduction to {productname}
:revdate: 2025-012-02
:page-revdate: {revdate}

[preface]
== About {productname}

WHAT?::
  AI hallucinations occurs when an LLM generates information that
  is not based on real-world facts or evidence. This can include
  fictional events, incorrect data or irrelevant outputs.

WHY?::
  Learn to create effective prompts that can help AI generate
  accurate and reliable content.

EFFORT::
  Less than 15 minutes of reading.

include::../tasks/AI-hallucinations-how-to-prevent.adoc[leveloffset=+1]
include::../references/AI-hallucinations-causes.adoc[leveloffset=+1]

[appendix]
include::../references/AI-glossary.adoc[leveloffset=+1]

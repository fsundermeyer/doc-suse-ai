[[nvidia-gpu-driver-installation-sles]]
= Installing {nvidia} GPU drivers on {sles}

[[nvidia-gpu-requirements-sles]]
== Requirements

If you are following this guide, it assumes that you have the following
already available:

* At least one host with {slsa} {sles-ai-version} installed, physical or
  virtual.
* Your hosts are attached to a subscription as this is required for
  package access.
* A link:https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus[compatible {nvidia} GPU] installed or fully passed through to the virtual
  machine in which {slsa} is running.
* Access to the {rootuser} user—these instructions assume you are
  the {rootuser} user, and not escalating your privileges via `sudo`.

[[nvidia-gpu-pre-install-sles]]
== Considerations before the installation

[[nvidia-gpu-install-generation-sles]]
=== Select the driver generation

You must verify the driver generation for the {nvidia} GPU that your
system has. For modern GPUs, the `G06` driver is the
most common choice. Find more details in
link:https://en.opensuse.org/SDB:NVIDIA_drivers#Install[the support database].

This section details the installation of the `G06`
generation of the driver.

[[nvidia-gpu-pre-install-additional-components-sles]]
=== Additional {nvidia} components

Besides the {nvidia} open-driver provided by {suse} as part of {slsa},
you might also need additional {nvidia} components. These could include
OpenGL libraries, {cuda} toolkits, command-line utilities such as
`nvidia-smi`, and container-integration components such
as nvidia-container-toolkit. Many of these components are not shipped by
{suse} as they are proprietary {nvidia} software. This section describes
how to configure additional repositories that give you access to these
components and provides examples of using these tools to achieve a fully
functional system.

[[nvidia-gpu-pre-install-procedure-sles]]
== The installation procedure

ifdef::deployment_airgap[]
. On the *remote* host, run the script
  `SUSE-AI-mirror-nvidia.sh` from the air-gapped stack
  (see xref:ai-air-gap-stack[]) to download all required
  {nvidia} RPM packages to a local directory, for example:
+
[source,bash]
----
$ SUSE-AI-mirror-nvidia.sh \
  -p _/LOCAL_MIRROR_DIRECTORY_ \
  -l https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64 \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/
----
+
After the download is complete, transfer the downloaded directory with
all its content to each GPU-enabled *local* host.
endif::[]

ifdef::deployment_standard[]
. Add a package repository from {nvidia}. This allows pulling in
  additional utilities, for example, `nvidia-smi`.
+
For the {x86-64} architecture, run:
+
[source,bash,subs="+attributes"]
----
# zypper ar \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ \
  cuda-sle15
# zypper --gpg-auto-import-keys refresh
----
+
For the {arm64} architecture, run:
+
[source,bash,subs="+attributes"]
----
# zypper ar \
  https://developer.download.nvidia.com/compute/cuda/repos/sles15/sbsa/ \
  cuda-sle15
transactional update # zypper --gpg-auto-import-keys refresh
----
endif::[]

. Install the Open Kernel driver KMP and detect the driver version.
+
[source,bash]
----
# zypper install -y --auto-agree-with-licenses \
  nv-prefer-signed-open-driver
# version=$(rpm -qa --queryformat '%{VERSION}\n' \
  nv-prefer-signed-open-driver | cut -d "_" -f1 | sort -u | tail -n 1)
----

. You can then install the appropriate packages for additional utilities
  that are useful for testing purposes.
+
[source,bash]
----
# zypper install -y --auto-agree-with-licenses \
nvidia-compute-utils-G06=${version} \
nvidia-persistenced=${version}
----

. Reboot the host to make the changes effective.
+
[source,bash,subs="+attributes"]
----
# reboot
----

. Log back in and use the `nvidia-smi` tool to verify
  that the driver is loaded successfully and that it can both access and
  enumerate your GPUs.
+
[source,bash,subs="+attributes"]
----
# nvidia-smi
----
+
The output of this command should show you something similar to the
following output. In the example below, the system has one GPU.
+
[source,subs="+attributes"]
----
Fri Aug  1 15:32:10 2025
+------------------------------------------------------------------------------+
| NVIDIA-SMI 580.82.07      Driver Version: 580.82.07    CUDA Version: 13.0    |
|------------------------------+------------------------+----------------------+
| GPU  Name      Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp Perf Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                              |                        |               MIG M. |
|==============================+========================+======================|
|   0  Tesla T4            On  |   00000000:00:1E.0 Off |                    0 |
| N/A   33C   P8   13W /   70W |       0MiB /  15360MiB |      0%      Default |
|                              |                        |                  N/A |
+------------------------------+------------------------+----------------------+

+------------------------------------------------------------------------------+
| Processes:                                                                   |
|  GPU   GI   CI        PID   Type   Process name                   GPU Memory |
|        ID   ID                                                    Usage      |
|==============================================================================|
|  No running processes found                                                  |
+------------------------------------------------------------------------------+
----

[[nvidia-gpu-validation-sles]]
== Validation of the driver installation

Running the `nvidia-smi` command has verified that, at
the host level, the {nvidia} device can be accessed and that the drivers
are loading successfully. To validate that it is functioning, you need to
validate that the GPU can take instructions from a user-space application,
ideally via a container and through the {cuda} library, as that is
typically what a real workload would use. For this, we can make a further
modification to the host OS by installing
`nvidia-container-toolkit`.

. Install the `nvidia-container-toolkit` package from
  the {nvidia} Container Toolkit repository.
+
[source,bash,subs="+attributes"]
----
# zypper ar \
"https://nvidia.github.io/libnvidia-container/stable/rpm/"\
nvidia-container-toolkit.repo
# zypper --gpg-auto-import-keys install \
  -y nvidia-container-toolkit
----
+
The `nvidia-container-toolkit.repo` file contains a
stable repository `nvidia-container-toolkit` and an
experimental repository
`nvidia-container-toolkit-experimental`. Use the
stable repository for production use. The experimental repository is
disabled by default.

. Verify that the system can successfully enumerate the devices using
  the {nvidia} Container Toolkit. The output should be verbose, with
  INFO and WARN messages, but no ERROR messages.
+
[source,bash,subs="+attributes"]
----
# nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
----
+
This ensures that any container started on the machine can employ
discovered {nvidia} GPU devices.

. You can then run a {podman}-based container. Doing this via
  `podman` gives you a good way of validating access to
  the {nvidia} device from within a container, which should give
  confidence for doing the same with {kube} at a later stage.
+
Give {podman} access to the labeled {nvidia} devices that were taken
care of by the previous command and simply run the
`bash` command.
+
[source,bash,subs="+attributes"]
----
# podman run --rm --device nvidia.com/gpu=all \
  --security-opt=label=disable \
  -it registry.suse.com/bci/bci-base:latest bash
----
+
You can now execute commands from within a temporary {podman}
container. It does not have access to your underlying system and is
*ephemeral*—whatever you change in the
container does not persist. Also, you cannot break anything on the
underlying host.

. Inside the container, install the required {cuda} libraries. Identify
  their version from the output of the `nvidia-smi`
  command. From the above example, we are installing {cuda} version 13.0
  with many examples, demos and development kits to fully validate the
  GPU.
+
[source,bash,subs="+attributes"]
----
# zypper ar \
  http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ \
  cuda-sle15-sp6
# zypper --gpg-auto-import-keys refresh
# zypper install -y cuda-libraries-13-0 cuda-demo-suite-12-9
----

. Inside the container, run the `deviceQuery` {cuda}
  example of the same version, which comprehensively validates GPU
  access via {cuda} and from within the container itself.
+
[source,subs="+attributes"]
----
# /usr/local/cuda-12.9/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API)

Detected 1 CUDA Capable device(s)

Device 0: "Tesla T4"
  CUDA Driver Version / Runtime Version          13.0/ 13.0
  CUDA Capability Major/Minor version number:    7.5
  Total amount of global memory:                 14913 MBytes (15637086208 bytes)
  (40) Multiprocessors, ( 64) CUDA Cores/MP:     2560 CUDA Cores
  GPU Max Clock rate:                            1590 MHz (1.59 GHz)
  Memory Clock rate:                             5001 Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 4194304 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1024
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 30
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 13.0, CUDA Runtime Version = 13.0, NumDevs = 1, Device0 = Tesla T4
Result = PASS
----
+
From inside the container, you can continue to run any other {cuda}
workload—such as compilers—to run further tests. When
finished, you can exit the container.
+
[source,bash,subs="+attributes"]
----
# exit
----
+
[IMPORTANT]
====
Changes you have made in the container and packages you have
installed inside will be lost and will not impact the underlying
operating system.
====

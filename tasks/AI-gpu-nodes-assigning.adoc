[#ai-gpu-nodes-assigning]
= Assigning GPU nodes to applications

When deploying a containerized application to {kube}, you need to ensure that containers requiring GPU resources are run on appropriate worker nodes.
For example, {ollama}, a core component of {productname}, can deeply benefit from the use of GPU acceleration.
This topic describes how to satisfy this requirement by explicitly requesting GPU resources and labeling worker nodes for configuring the node selector.

.Requirements
* {kube} cluster--such as {rke2}--must be available and configured with more than one worker node in which certain nodes have {nvidia} GPU resources and others do not.
* This document assumes that any kind of deployment to the {kube} cluster is done using {helm} charts.

[#ai-gpu-node-labeling]
== Labeling GPU nodes

To distinguish nodes with the GPU support from non-GPU nodes, {kube} uses *labels*.
Labels are used for relevant metadata and should not be confused with annotations that provide simple information about a resource.
It is possible to manipulate labels with the `kubectl` command, as well as by tweaking configuration files from the nodes.
If an IaC tool such as Terraform is used, labels can be inserted in the node resource configuration files.

To label a single node, use the following command:

[source,bash,subs="+attributes"]
----
{prompt_user}kubectl label node <GPU_NODE_NAME> accelerator=nvidia-gpu
----

To achieve the same result by tweaking the `node.yaml` node configuration, add the following content and apply the changes with `kubectl apply -f node.yaml`:

[source,yaml]
----
apiVersion: v1
kind: Node
metadata:
  name: node-name
  labels:
    accelerator: nvidia-gpu
----

[TIP]
.Labeling multiple nodes
====
To label multiple nodes, use the following command:

[source,bash,subs="+attributes"]
----
{prompt_user}kubectl label node \
  <GPU_NODE_NAME1> \
  <GPU_NODE_NAME2> ... \
  accelerator=nvidia-gpu
----
====

[TIP]
====
If Terraform is being used as an IaC tool, you can add labels to a group of nodes by editing the `.tf` files and adding the following values to a resource:

[source]
----
resource "node_group" "example" {
  labels = {
    "accelerator" = "nvidia-gpu"
  }
}
----
====

To check if the labels are correctly applied, use the following command:

[source,bash,subs="+attributes"]
----
{prompt_user}kubectl get nodes --show-labels
----

[#ai-gpu-node-assign]
== Assigning GPU nodes

The matching between a container and a node is configured by the explicit resource allocation and the use of labels and node selectors.
The use cases described below focus on {nvidia} GPUs.

[#ai-gpu-passthru]
=== Enable GPU passthrough

Containers are isolated from the host environment by default.
For the containers that rely on the allocation of GPU resources, their {helm} charts must enable GPU passthrough so that the container can access and use the GPU resource.
Without enabling the GPU passthrough, the container may still run, but it can only use the main CPU for all computations.
Refer to link:https://documentation.suse.com/suse-ai/1.0/html/AI-deployment-intro/index.html#ollama-helmchart[{ollama} {helm} chart] for an example of the configuration required for GPU acceleration.

[#ai-gpu-assign-request-resources]
=== Assignment by resource request

After the {nvoperator} is configured on a node, you can instantiate applications requesting the resource `nvidia.com/gpu` provided by the operator.
Add the following content to your `values.yaml` file.
Specify the number of GPUs according to your setup.

[source,yaml]
----
resources:
  requests:
    nvidia.com/gpu: 1
  limits:
    nvidia.com/gpu: 1
----

[#ai-gpu-assign-labels]
=== Assignment by labels and node selectors

If affected cluster nodes are labeled with a label such as `accelerator=nvidia-gpu`, you can configure the node selector to check for the label.
In this case, use the following values in your `values.yaml` file.

[source,yaml]
----
nodeSelector:
  accelerator: nvidia-gpu
----

[#ai-gpu-assign-verify]
== Verifying {ollama} GPU assignment

If the GPU is correctly detected, the {ollama} container logs this event:

[source]
----
| [...] source=routes.go:1172 msg="Listening on :11434 (version 0.0.0)"                                              │
│ [...] source=payload.go:30 msg="extracting embedded files" dir=/tmp/ollama2502346830/runners                       │
│ [...] source=payload.go:44 msg="Dynamic LLM libraries [cuda_v12 cpu cpu_avx cpu_avx2]"                             │
│ [...] source=gpu.go:204 msg="looking for compatible GPUs"                                                          │
│ [...] source=types.go:105 msg="inference compute" id=GPU-c9ad37d0-d304-5d2a-c2e6-d3788cd733a7 library=cuda compute │
----
